{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:04:53.101702Z",
     "start_time": "2021-11-25T11:04:52.007522Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from typing import List\n",
    "\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from create_data import recreate_databases, create_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:05:06.591181Z",
     "start_time": "2021-11-25T11:04:53.104702Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DM\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:05:40.734775Z",
     "start_time": "2021-11-25T11:05:06.596174Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 12:05:06.603 | INFO     | create_data:recreate_databases:28 - Removing spark warehouse (path = 'spark-warehouse')\n",
      "2021-11-25 12:05:06.650 | INFO     | create_data:recreate_databases:32 - Creating database 'standardized_glovo_live'\n",
      "2021-11-25 12:05:10.775 | INFO     | create_data:recreate_databases:32 - Creating database 'mpcustomer_custom_events'\n",
      "2021-11-25 12:05:10.871 | INFO     | create_data:recreate_databases:32 - Creating database 'mpcustomer_screen_views'\n",
      "2021-11-25 12:05:10.957 | INFO     | create_data:recreate_databases:32 - Creating database 'enriched_custom_events'\n",
      "2021-11-25 12:05:11.010 | INFO     | create_data:recreate_databases:32 - Creating database 'enriched_screen_views'\n",
      "2021-11-25 12:05:11.082 | INFO     | create_data:create_table:63 - Creating table 'standardized_glovo_live.cities'\n",
      "2021-11-25 12:05:23.506 | INFO     | create_data:create_table:63 - Creating table 'standardized_glovo_live.devices'\n",
      "2021-11-25 12:05:32.152 | INFO     | create_data:create_table:63 - Creating table 'mpcustomer_custom_events.order_created'\n"
     ]
    }
   ],
   "source": [
    "recreate_databases(spark)\n",
    "create_tables(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:07:01.707174Z",
     "start_time": "2021-11-25T11:07:01.672164Z"
    }
   },
   "outputs": [],
   "source": [
    "class Loader(ABC):\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "        self.sdf = self.load()\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self) -> DataFrame:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LoaderLiveDB(Loader):\n",
    "    database_in = \"standardized_glovo_live\"\n",
    "    \n",
    "class LoaderCustomEvent(Loader):\n",
    "    database_in = \"mpcustomer_custom_events\"\n",
    "    \n",
    "class LoaderScreenView(Loader):\n",
    "    database_in = \"mpcustomer_screen_views\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:07:02.602037Z",
     "start_time": "2021-11-25T11:07:02.582028Z"
    }
   },
   "outputs": [],
   "source": [
    "class Writer(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def write(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class WriterCustomEvent(Writer):\n",
    "    database_out = \"enriched_custom_events\"\n",
    "\n",
    "class WriterScreenView(Writer):\n",
    "    database_out = \"enriched_screen_views\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:07:45.924973Z",
     "start_time": "2021-11-25T11:07:45.909817Z"
    }
   },
   "outputs": [],
   "source": [
    "class CitiesPort(LoaderLiveDB):\n",
    "    code = \"code\"\n",
    "    time_zone = \"time_zone\"\n",
    "    country_code = \"country_code\"\n",
    "    \n",
    "    def load(self) -> DataFrame:\n",
    "        sdf = self.spark.table(f\"{self.database_in}.cities\")\n",
    "        return sdf.select(self.code, self.time_zone, self.country_code)\n",
    "\n",
    "\n",
    "class DevicesPort(LoaderLiveDB):\n",
    "    device_id = \"custom_attributes__device_id\"\n",
    "    experiment_score = \"device_experiment_score\"\n",
    "\n",
    "    def load(self) -> DataFrame:\n",
    "        sdf = self.spark.table(f\"{self.database_in}.devices\")\n",
    "        return sdf.selectExpr(\n",
    "            f\"id AS {self.device_id}\",\n",
    "            f\"experiment_score AS {self.experiment_score}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:09:23.616711Z",
     "start_time": "2021-11-25T11:09:23.592672Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomEventPort(LoaderCustomEvent, WriterCustomEvent):\n",
    "    \n",
    "    creation_date = \"p_creation_date\"\n",
    "    city = \"custom_attributes__city\"\n",
    "    \n",
    "    def __init__(self, spark, exec_date: date, n_days: int):\n",
    "        self.exec_date = exec_date\n",
    "        self.n_days = n_days\n",
    "\n",
    "        super().__init__(spark)\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load(self) -> DataFrame:\n",
    "        \n",
    "        start = self.exec_date - timedelta(days=self.n_days)\n",
    "        end = self.exec_date\n",
    "        \n",
    "        sdf = self.spark.table(f\"{self.database_in}.{self.name}\")\n",
    "\n",
    "        return sdf.filter(\n",
    "            f\"{self.creation_date} BETWEEN '{start:%Y-%m-%d}' AND '{end:%Y-%m-%d}'\"\n",
    "        )\n",
    "\n",
    "    def write(self):\n",
    "        self.sdf.write.format(\"parquet\").saveAsTable(f\"{self.database_out}.{self.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:09:29.566735Z",
     "start_time": "2021-11-25T11:09:29.548725Z"
    }
   },
   "outputs": [],
   "source": [
    "class OrderCreatedPort(CustomEventPort):\n",
    "    name = \"order_created\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:09:58.623972Z",
     "start_time": "2021-11-25T11:09:58.612166Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformation(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def apply(self, sdf: DataFrame) -> DataFrame:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Table(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def sdf(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:10:24.571240Z",
     "start_time": "2021-11-25T11:10:24.546241Z"
    }
   },
   "outputs": [],
   "source": [
    "class AddTimezone(Transformation):\n",
    "    def __init__(self, cities: CitiesPort):\n",
    "        self.cities = cities\n",
    "\n",
    "    def apply(self, table: Table) -> Table:\n",
    "        \n",
    "        table.sdf = table.sdf.join(\n",
    "            F.broadcast(self.cities.sdf),\n",
    "            on=table.sdf[table.city] == self.cities.sdf[self.cities.code],\n",
    "            how=\"left\",\n",
    "        ).drop(self.cities.code)\n",
    "\n",
    "        return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:10:44.405936Z",
     "start_time": "2021-11-25T11:10:44.388937Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformLinearly:\n",
    "    \n",
    "    def __init__(self, table: Table, transformations: List[Transformation]):\n",
    "        self.table = table\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def run(self):\n",
    "        for transformation in self.transformations:\n",
    "            self.table = transformation.apply(self.table)\n",
    "\n",
    "        self.table.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:11:11.468453Z",
     "start_time": "2021-11-25T11:11:07.917517Z"
    }
   },
   "outputs": [],
   "source": [
    "order_created = OrderCreatedPort(spark, exec_date=date(2021, 11, 19), n_days=3)\n",
    "transformations = [\n",
    "    AddTimezone(CitiesPort(spark))\n",
    "]\n",
    "\n",
    "order_created_job = TransformLinearly(\n",
    "    table=order_created,\n",
    "    transformations=transformations,\n",
    ")\n",
    "order_created_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T11:11:12.519640Z",
     "start_time": "2021-11-25T11:11:11.475452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------+-------------+------------+\n",
      "|custom_attributes__city|p_creation_date|    time_zone|country_code|\n",
      "+-----------------------+---------------+-------------+------------+\n",
      "|                    BCN|     2021-11-19|Europe/Madrid|          ES|\n",
      "|                    BCN|     2021-11-18|Europe/Madrid|          ES|\n",
      "|                    CAG|     2021-11-17|  Europe/Rome|          IT|\n",
      "+-----------------------+---------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(f\"{order_created.database_out}.{order_created.name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
